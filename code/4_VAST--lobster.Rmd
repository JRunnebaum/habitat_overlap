---
title: Example script for VAST for spatio-temporal analysis of single-species catch-rate
  data
author: "James Thorson"
date: "October 10, 2016"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
html_document:
  toc: yes
---


```{r set_options, echo=FALSE, message=FALSE, warning=FALSE}
# Width should apply to tidy
# digits hopefully affects number of digits when using print
options(width=50, width.cutoff=50, digits = 3) 
install.packages("pander", repos="http://cran.us.r-project.org")
```

```{r wrap-hook, echo=FALSE}
# FROM: https://github.com/yihui/knitr-examples/blob/master/077-wrap-output.Rmd
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
# TRIGGERED USING `linewidth=60`
```


# Overview
This tutorial will walk through a simple example of how to use `VAST` for estimating single-species abundance indices, distribution shifts, and range expansion.

# Getting started

To install TMB on a windows machine, we need to first install [Rtools](https://cran.r-project.org/bin/windows/Rtools/).  During the installation, please select the option to have Rtools included in your system path.  On other operating systems, it is not necessary to install Rtools.  We then install `VAST`  
```{r load_packages, message=FALSE}
devtools::install_github("james-thorson/VAST") 
devtools::install_github("james-thorson/utilities")
# devtools::install_github("nwfsc-assess/geostatistical_delta-GLMM@*release")
```

Next load libraries.
```{r load_libraries, message=FALSE, warning=FALSE}
library(TMB)               # Can instead load library(TMBdebug)
library(VAST)
library(dplyr)
```

## Further information

If you have further questions after reading this tutorial, please explore the [GitHub repo](https://github.com/james-thorson/VAST/#description) mainpage, wiki, and glossary.  Also please explore the R help files, e.g., `?Data_Fn` for explanation of data inputs, or `?Param_Fn` for explanation of parameters.  

## Related tools

Related tools for spatio-temporal fisheries analysis are currently housed at [www.FishStats.org](http://www.FishStats.org).  These include [SpatialDeltaGLMM](https://github.com/nwfsc-assess/geostatistical_delta-GLMM/#description), a single-species antecedent of VAST, and [www.FishViz.org](http://www.FishViz.org), a tool for visualizing single-species results using worldwide. `VAST` and `SpatialDeltaGLMM` both use continuous integration to confirm that they give identical estimates when applied to single-species data.  

## How to cite VAST

`VAST` has involved many publications for developing individual features.  If using `VAST`, please read and cite:

```{r citation, tidy=TRUE, width=70, width.cutoff=70}
citation("VAST")
```

and also browse the [GitHub list](https://github.com/james-thorson/VAST/#description-of-package) of papers

# Settings
First chose an example data set for this script, as archived with package
```{r, tidy=TRUE, linewidth=60}
# spring.lob <- read.csv("../data/lobster_data_for_VAST/spring_lobster_all.csv")
fall.lob <- read.csv("../data/lobster_data_for_VAST/fall_lobster_all.csv")

#WHAT DATA DO YOU WANT TO RUN?
# Data_Set <- spring.lob
Data_Set <- fall.lob

Data_Set <- Data_Set %>% 
  filter(!is.na(area_swept),
         year <= 2015)

summary(Data_Set)

Data_Set$Q <- as.factor(Data_Set$survey)
table(Data_Set$Q)
Q_ik <- ThorsonUtilities::vector_to_design_matrix(Data_Set$Q)
Q_ik <- as.matrix(Q_ik[,2:3])
head(Q_ik)

```

Next use latest version for CPP code
```{r}
# Version = "VAST_v5_4_0" 
Version = get_latest_version( package="VAST" )

```

## Spatial settings
The following settings define the spatial resolution for the model, and whether to use a grid or mesh approximation
```{r}
Method = c("Grid", "Mesh", "Spherical_mesh")[2]
grid_size_km = 25
n_x = c(100, 250, 500, 1000, 2000)[2] # Number of stations
Kmeans_Config = list( "randomseed"=1, "nstart"=100, "iter.max"=1e3 )    
```

## Model settings
The following settings define whether to include spatial and spatio-temporal variation, whether its autocorrelated, and whether there's overdispersion
```{r, tidy=TRUE}
FieldConfig = c("Omega1"=1, "Epsilon1"=1, "Omega2"=1, "Epsilon2"=1) 
RhoConfig = c("Beta1"=2, "Beta2"=2, "Epsilon1"=0, "Epsilon2"=0) 
OverdispersionConfig = c("Delta1"=0, "Delta2"=0)
ObsModel = c(2,0)  
```

## Potential outputs
The following settings define what types of output we want to calculate
```{r, tidy=TRUE}
Options =  c("SD_site_density"=1, "SD_site_logdensity"=0, "Calculate_Range"=1, "Calculate_evenness"=0, "Calculate_effective_area"=1, "Calculate_Cov_SE"=0, 'Calculate_Synchrony'=0, 'Calculate_Coherence'=0)
```

## Stratification for results

We also define any potential stratification of results, and settings specific to any case-study data set
```{r define_strata, tidy=TRUE, linewidth=50}
# Default
strata.limits <- data.frame('STRATA'=c(464, 465, 466, 467, 511, 512, 513, 514, 515, 521, 522, 525, 526, 551, 552, 561, 562))


```

## Derived objects

Depending on the case study, we define a `Region` used when extrapolating or plotting density estimates.  If its a different data set, it will define `Region="Other"`, and this is a recognized level for all uses of `Region` (which attempts to define reasonable settings based on the location of sampling).  For example `Data_Set="Iceland_cod"` has no associated meta-data for the region, so it uses `Region="Other"` by default.
```{r define_region, tidy=FALSE}
Region = "NWA" #this is for the predefined extrapolation package
# Region = "Northwest_Atlantic" #this is for my custom extrapolation 
```

## Save settings

We then set the location for saving files.
```{r make_dir, message=FALSE, warning=FALSE}

DateFile = paste0(getwd(),'/VAST_output/')
  dir.create(DateFile)
```

I also like to save all settings for later reference, although this is not necessary.
```{r, tidy=TRUE, linewidth=50}
Record = ThorsonUtilities::bundlelist( c("Data_Set","Version","Method","grid_size_km","n_x","FieldConfig","RhoConfig","OverdispersionConfig","ObsModel","Kmeans_Config","Q_ik") )
save( Record, file=file.path(DateFile,"Record.RData"))
capture.output( Record, file=paste0(DateFile,"Record.txt"))

```

# Prepare the data

## Data-frame for catch-rate data

Depending upon the `Data_Set` chosen, we load archived data sets that are distributed with the package. Each archived data set is then reformatted to create a data-frame `Data_Geostat` with a standardized set of columns. For a new data set, the user is responsible for formatting `Data_Geostat` appropriately to match this format.  We show the first six rows of `Data_Geostat` given that Data_Set = `Data_Set`.  
```{r load_data, echo=FALSE, message=FALSE}
DF <- Data_Set

table(Data_Set$Year)
Data_Geostat = data.frame( "Year"=DF[,"year"], 
                           "Strata"= DF[,"stat_area"],
                           "Catch_KG"=DF[,"catch_n_a"], 
                           "AreaSwept_km2"=DF[,"area_swept"], 
                           "Vessel"=DF[,"survey"], 
                           "Lat"=DF[,"lat"], 
                           "Lon"=DF[,"lon"] )


```

```{r show_data_head, results="asis", echo=FALSE}
pander::pandoc.table( Data_Geostat[1:6,], digits=3 )
```




```{r}
## Extrapolation grid

make_extrapolation_info <- function (strata.limits = NULL, observations_LL, zone = NA, ...) {
  load("../data/grid/lobster_grid_GB_GOM_BODCdepth_SEDcat_area_swept_strata_noNA.Rda")
  


{
    if (is.null(strata.limits)) {
        strata.limits = list(All_areas = 1:1e+05)
    }
    message("Using strata ", strata.limits)
   
    Data_Extrap <- grid_no_na
    Area_km2_x = Data_Extrap[, "Area_in_survey_km2"]
    Tmp = cbind(BEST_DEPTH_M = 0, BEST_LAT_DD = Data_Extrap[, 
        "Lat"], BEST_LON_DD = Data_Extrap[, "Lon"])
    if (length(strata.limits) == 1 && strata.limits[1] == "EPU") {
        a_el = matrix(NA, nrow = nrow(Data_Extrap), ncol = length(unique(northwest_atlantic_grid[, 
            "EPU"])), dimnames = list(NULL, unique(northwest_atlantic_grid[, 
            "EPU"])))
        for (l in 1:ncol(a_el)) {
            a_el[, l] = ifelse(Data_Extrap[, "EPU"] == unique(northwest_atlantic_grid[, 
                "EPU"])[l], Area_km2_x, 0)
        }
    }
    else {
        a_el = as.data.frame(matrix(NA, nrow = nrow(Data_Extrap), 
            ncol = length(strata.limits), dimnames = list(NULL, 
                names(strata.limits))))
        for (l in 1:ncol(a_el)) {
            a_el[, l] = ifelse(Data_Extrap[, "stat_area"] %in% 
                strata.limits[[l]], Area_km2_x, 0)
        }
    }
    tmpUTM = FishStatsUtils::Convert_LL_to_UTM_Fn(Lon = Data_Extrap[, 
        "Lon"], Lat = Data_Extrap[, "Lat"], zone = zone)
    Data_Extrap = cbind(Data_Extrap, Include = 1)
    Data_Extrap[, c("E_km", "N_km")] = tmpUTM[, c("X", "Y")]
    Return = list(a_el = a_el, Data_Extrap = Data_Extrap, zone = attr(tmpUTM, 
        "zone"), flip_around_dateline = FALSE, Area_km2_x = Area_km2_x)
    return(Return)
  }
}
```


  
We also generate the extrapolation grid appropriate for a given region.  For new regions, we use `Region="Other"`.
```{r extrapolation_grid, message=FALSE, tidy=TRUE, linewidth=60}

#our own extrapolation grid
if( Region == "NWA" ){
    Extrapolation_List = make_extrapolation_info( Region = Region,
      strata.limits=strata.limits, observations_LL=Data_Geostat[,c('Lat','Lon')] )
  }

```

## Derived objects for spatio-temporal estimation

And we finally generate the information used for conducting spatio-temporal parameter estimation, bundled in list `Spatial_List`
```{r spatial_information, message=FALSE, warning=FALSE, tidy=TRUE, linewidth=60}
Spatial_List = make_spatial_info( grid_size_km=grid_size_km, n_x=n_x, Method=Method, Lon=Data_Geostat[,'Lon'], Lat=Data_Geostat[,'Lat'], Extrapolation_List=Extrapolation_List, DirPath=DateFile, Save_Results=FALSE )
# Add knots to Data_Geostat
Data_Geostat = cbind( Data_Geostat, "knot_i"=Spatial_List$knot_i )
```

# Build and run model

## Build model

To estimate parameters, we first build a list of data-inputs used for parameter estimation.  `Data_Fn` has some simple checks for buggy inputs, but also please read the help file `?Data_Fn`.  
```{r build_data, message=FALSE, tidy=TRUE, linewidth=60}
TmbData = make_data("Version"=Version, "FieldConfig"=FieldConfig, "OverdispersionConfig"=OverdispersionConfig, "RhoConfig"=RhoConfig, "ObsModel"=ObsModel, "c_i"=rep(0,nrow(Data_Geostat)), "b_i"=Data_Geostat[,'Catch_KG'], "a_i"=Data_Geostat[,'AreaSwept_km2'], "v_i"=as.numeric(Data_Geostat[,'Vessel'])-1, "s_i"=Data_Geostat[,'knot_i']-1, "t_i"=Data_Geostat[,'Year'], "a_xl"=Spatial_List$a_xl, "MeshList"= Spatial_List$MeshList, "GridList"=Spatial_List$GridList, "Method"=Spatial_List$Method, "Options"=Options, "Q_ik"= Q_ik)

```

We then build the TMB object.
```{r build_object, message=FALSE, results="hide", tidy=TRUE}
TmbList = make_model("TmbData"=TmbData, "RunDir"=DateFile, "Version"=Version, "RhoConfig"=RhoConfig, "loc_x"=Spatial_List$loc_x, "Method"=Spatial_List$Method)
Obj = TmbList[["Obj"]]

```

## Estimate fixed effects and predict random effects

Next, we use a gradient-based nonlinear minimizer to identify maximum likelihood estimates for fixed-effects
```{r estimate_parameters, results="hide", tidy=TRUE}
Opt = TMBhelper::Optimize( obj=Obj, lower=TmbList[["Lower"]], upper=TmbList[["Upper"]], getsd=TRUE, savedir=DateFile, bias.correct=TRUE, newtonsteps=1, bias.correct.control=list(sd=FALSE, split=NULL, nsplit=1, vars_to_correct="Index_cyl") )
```

Finally, we bundle and save output
```{r save_results, linewidth=60}
Report = Obj$report()
Save = list("Opt"=Opt, "Report"=Report, "ParHat"=Obj$env$parList(Opt$par), "TmbData"=TmbData)
save(Save, file=paste0(DateFile,"Save.RData"))
```

# Diagnostic plots

We first apply a set of standard model diagnostics to confirm that the model is reasonable and deserves further attention.  If any of these do not look reasonable, the model output should not be interpreted or used.

## Plot data

It is always good practice to conduct exploratory analysis of data.  Here, I visualize the spatial distribution of data.  Spatio-temporal models involve the assumption that the probability of sampling a given location is statistically independent of the probability distribution for the response at that location.  So if sampling "follows" changes in density, then the model is probably not appropriate!
```{r explore_data, results="hide", tidy=TRUE, message=FALSE, warning=FALSE}
plot_data(Extrapolation_List=Extrapolation_List, Spatial_List=Spatial_List, Data_Geostat=Data_Geostat, PlotDir=DateFile )
```
![Spatial extent and location of knots](VAST_output/Data_and_knots.png) 

![Spatial distribution of catch-rate data](VAST_output/Data_by_year.png) 

## Convergence
Here I print the diagnostics generated during parameter estimation, and I confirm that (1) no parameter is hitting an upper or lower bound and (2) the final gradient for each fixed-effect is close to zero. For explanation of parameters, please see `?Data_Fn`.
```{r print_results, results="asis"}
pander::pandoc.table( Opt$diagnostics[,c('Param','Lower','MLE','Upper','final_gradient')] ) 
```

## Diagnostics for encounter-probability component

Next, we check whether observed encounter frequencies for either low or high probability samples are within the 95% predictive interval for predicted encounter probability
```{r diagnostics_encounter_prob, results="hide", eval=TRUE, tidy=TRUE, linewidth=50}
Enc_prob = plot_encounter_diagnostic( Report=Report, Data_Geostat=Data_Geostat, DirName=DateFile)
```
![Expectated probability and observed frequency of encounter for "encounter probability" component](VAST_output/Diag--Encounter_prob.png) 


## Diagnostics for positive-catch-rate component

We can visualize fit to residuals of catch-rates given encounters using a Q-Q plot.  A good Q-Q plot will have residuals along the one-to-one line.  
```{r plot_QQ, eval=TRUE, tidy=TRUE, linewidth=50, message=FALSE, warning=FALSE}
Q = plot_quantile_diagnostic( TmbData=TmbData, Report=Report, FileName_PP="Posterior_Predictive",
FileName_Phist="Posterior_Predictive-Histogram", 
FileName_QQ="Q-Q_plot", FileName_Qhist="Q-Q_hist", DateFile=DateFile )
 # SpatialDeltaGLMM::
```
![Quantile-quantile plot indicating residuals for "positive catch rate" component](VAST_output/Q-Q_plot.jpg) 

## Diagnostics for plotting residuals on a map

Finally, we visualize residuals on a map.  To do so, we first define years to plot and generate plotting inputs.
useful plots by first determining which years to plot (`Years2Include`), and labels for each plotted year (`Year_Set`)
```{r plot_years}
# Get region-specific settings for plots
MapDetails_List = make_map_info( "Region"=Region, "NN_Extrap"=Spatial_List$PolygonList$NN_Extrap, "Extrapolation_List"=Extrapolation_List )
# Decide which years to plot                                                   
Year_Set = seq(min(Data_Geostat[,'Year']),max(Data_Geostat[,'Year']))
Years2Include = which( Year_Set %in% sort(unique(Data_Geostat[,'Year'])))

```

We then plot Pearson residuals.  If there are visible patterns (areas with consistently positive or negative residuals accross or within years) then this is an indication of the model "overshrinking" results towards the intercept, and model results should then be treated with caution.  
```{r plot_pearson_resid, message=FALSE, warning=FALSE, tidy=TRUE, linewidth=50}
plot_residuals(Lat_i=Data_Geostat[,'Lat'], Lon_i=Data_Geostat[,'Lon'], TmbData=TmbData, Report=Report, Q=Q, savedir=DateFile, MappingDetails=MapDetails_List[["MappingDetails"]], PlotDF=MapDetails_List[["PlotDF"]], MapSizeRatio=MapDetails_List[["MapSizeRatio"]], Xlim=MapDetails_List[["Xlim"]], Ylim=MapDetails_List[["Ylim"]], FileName=DateFile, Year_Set=Year_Set, Years2Include=Years2Include, Rotate=MapDetails_List[["Rotate"]], Cex=MapDetails_List[["Cex"]], Legend=MapDetails_List[["Legend"]], zone=MapDetails_List[["Zone"]], mar=c(0,0,2,0), oma=c(3.5,3.5,0,0), cex=1.8)


```
![Pearson residuals for encounter-probability by knot](VAST_output/maps--encounter_pearson_resid.png) 

![Pearson residuals for positive catch rates by knot](VAST_output/maps--catchrate_pearson_resid.png) 


## Model selection

To select among models, we recommend using the Akaike Information Criterion, AIC, via `Opt$AIC=` ``r Opt$AIC``. 

# Model output

Last but not least, we generate pre-defined plots for visualizing results

## Direction of "geometric anisotropy"

We can visualize which direction has faster or slower decorrelation (termed "geometric anisotropy")
```{r plot_aniso, message=FALSE, results="hide", tidy=TRUE}
plot_anisotropy( FileName=paste0(DateFile,"Aniso.png"), Report=Report, TmbData=TmbData )
```
![Decorrelation distance for different directions](VAST_output/Aniso.png) 

## Density surface for each year

We can visualize many types of output from the model.  Here I only show predicted density, but other options are obtained via other integers passed to `plot_set` as described in `?PlotResultsOnMap_Fn`
plot_maps(plot_set=c(5) = Log-predicted density (rescaled) - this does not look right
plot_maps(plot_set=c() = Log-predicted density - 
```{r plot_density, message=FALSE, warning=FALSE, tidy=TRUE, linewidth=50}
Dens_xt = plot_maps(plot_set=c(3), MappingDetails=MapDetails_List[["MappingDetails"]], Report=Report, Sdreport=Opt$SD, PlotDF=MapDetails_List[["PlotDF"]], MapSizeRatio=MapDetails_List[["MapSizeRatio"]], Xlim=MapDetails_List[["Xlim"]], Ylim=MapDetails_List[["Ylim"]], FileName=DateFile, Year_Set=Year_Set, Years2Include=Years2Include, Rotate=MapDetails_List[["Rotate"]], Cex=MapDetails_List[["Cex"]], Legend=MapDetails_List[["Legend"]], zone=MapDetails_List[["Zone"]], mar=c(0,0,2,0), oma=c(3.5,3.5,0,0), cex=1.8, plot_legend_fig=FALSE)
```
![Density maps for each year](VAST_output/Dens.png) 

We can also extract density predictions at different locations, for use or plotting in other software. This is output in UTM using zone `r Extrapolation_List$zone-ifelse(Extrapolation_List$flip_around_dateline,30,0)`
```{r calc_density_dataframe, message=FALSE, warning=FALSE, tidy=TRUE, linewidth=50}
Dens_DF = cbind( "Density"=as.vector(Dens_xt), "Year"=Year_Set[col(Dens_xt)], "E_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'E_km'], "N_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'N_km'] )
```

```{r show_density_head, results="asis", echo=FALSE}
pander::pandoc.table( Dens_DF[1:6,], digits=3 )
```

## Index of abundance

The index of abundance is generally most useful for stock assessment models.
```{r plot_index, message=FALSE, tidy=TRUE, linewidth=50, results="asis"}
Index = plot_biomass_index( DirName=DateFile, TmbData=TmbData, Sdreport=Opt[["SD"]], Year_Set=Year_Set, Years2Include=Years2Include, use_biascorr=TRUE )
pander::pandoc.table( Index$Table[,c("Year","Fleet","Estimate_metric_tons","SD_log","SD_mt")] ) 
```
![Index of abundance plus/minus 1 standard error](VAST_output/Index.png) 

## Center of gravity and range expansion/contraction

We can detect shifts in distribution or range expansion/contraction.  
```{r plot_range, message=FALSE, tidy=TRUE, linewidth=50}
plot_range_index(Report=Report, TmbData=TmbData, Sdreport=Opt[["SD"]], Znames=colnames(TmbData$Z_xm), PlotDir=DateFile, Year_Set=Year_Set)

```
![Center of gravity (COG) indicating shifts in distribution plus/minus 1 standard error](VAST_output/center_of_gravity.png) 

![Effective area occupied indicating range expansion/contraction plus/minus 1 standard error](VAST_output/Effective_Area.png) 

tau<-subset(Opt$diagnostic$Param=="L_omega1_z")

sigma.om1<=1/(L_omega1_z*exp(logkappa1)*sqrt(4*pi)
